# %% [markdown]
# # SEN12FLOODS Dataset - Exploratory Data Analysis (EDA)
# 
# ## Má»¥c tiÃªu vÃ  Tá»•ng quan
# 
# **SEN12FLOODS** lÃ  dataset vá» flood detection sá»­ dá»¥ng dá»¯ liá»‡u vá»‡ tinh Sentinel-1 vÃ  Sentinel-2. Notebook nÃ y thá»±c hiá»‡n phÃ¢n tÃ­ch khÃ¡m phÃ¡ dá»¯ liá»‡u (EDA) Ä‘á»ƒ hiá»ƒu rÃµ:
# 
# ### Má»¥c tiÃªu chÃ­nh:
# 1. **KhÃ¡m phÃ¡ cáº¥u trÃºc dataset** - Hiá»ƒu tá»• chá»©c files vÃ  folders
# 2. **PhÃ¢n tÃ­ch sensor data** - So sÃ¡nh Sentinel-1 (SAR) vs Sentinel-2 (Optical)
# 3. **Data cleaning** - Loáº¡i bá» invalid files Ä‘á»ƒ cáº£i thiá»‡n training
# 4. **Visualization** - Táº¡o biá»ƒu Ä‘á»“ vÃ  sample images
# 5. **Export clean dataset** - Chuáº©n bá»‹ data sáº¡ch cho training
# 
# ### Vá» Sensor Data:
# - **Sentinel-1**: SAR (Synthetic Aperture Radar) - hoáº¡t Ä‘á»™ng 24/7, khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi mÃ¢y
# - **Sentinel-2**: Optical imagery - cháº¥t lÆ°á»£ng hÃ¬nh áº£nh cao, nhiá»u spectral bands
# 
# ### Workflow:
# 1. Cháº¡y notebook nÃ y TRÆ¯á»šC Ä‘á»ƒ clean data
# 2. Sau Ä‘á»ƒ train models
# 3. Training script sáº½ tá»± Ä‘á»™ng sá»­ dá»¥ng clean dataset
# 
# ---
# 

# %% [markdown]
# ## 1. Import Libraries vÃ  Setup
# 
# Import táº¥t cáº£ thÆ° viá»‡n cáº§n thiáº¿t vÃ  thiáº¿t láº­p cáº¥u hÃ¬nh cÆ¡ báº£n:
# 
# ### Libraries Ä‘Æ°á»£c sá»­ dá»¥ng:
# - **pandas, numpy**: Data manipulation vÃ  numerical computing
# - **matplotlib, seaborn**: Visualization vÃ  plotting  
# - **rasterio**: Äá»c/xá»­ lÃ½ dá»¯ liá»‡u GeoTIFF (satellite images)
# - **cv2, PIL**: Image processing
# - **glob, os, json**: File operations vÃ  metadata handling
# 

# %%
import os
import glob
import json
import random
import pickle
from collections import Counter

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import rasterio
import cv2
from PIL import Image

# Set style cho plots - táº¡o giao diá»‡n Ä‘áº¹p hÆ¡n
plt.style.use('default')
sns.set_palette("husl")

# Set random seeds cho reproducibility - Ä‘áº£m báº£o káº¿t quáº£ nháº¥t quÃ¡n
seed = 42
random.seed(seed)
np.random.seed(seed)

print("=" * 60)
print("SEN12FLOODS DATASET - EXPLORATORY DATA ANALYSIS")
print("=" * 60)


# %% [markdown]
# ## 2. Cáº¥u hÃ¬nh ÄÆ°á»ng dáº«n vÃ  KhÃ¡m phÃ¡ Cáº¥u trÃºc Dá»¯ liá»‡u
# 
# Trong cell nÃ y, chÃºng ta sáº½:
# 
# ### Má»¥c tiÃªu:
# 1. **Thiáº¿t láº­p data path** - ÄÆ°á»ng dáº«n Ä‘áº¿n SEN12FLOODS dataset
# 2. **Scan táº¥t cáº£ files** - TÃ¬m vÃ  Ä‘áº¿m cÃ¡c file .tif trong dataset
# 3. **PhÃ¢n tÃ­ch structure** - Hiá»ƒu cÃ¡ch tá»• chá»©c folders vÃ  files
# 
# ### Cáº¥u trÃºc Dataset:
# - Má»—i **folder** Ä‘áº¡i diá»‡n cho má»™t **scene/region** cá»¥ thá»ƒ
# - Má»—i scene chá»©a nhiá»u **bands** tá»« Sentinel-1 vÃ  Sentinel-2
# - Files Ä‘Æ°á»£c tá»• chá»©c theo **sensor type** vÃ  **spectral bands**
# 

# %%
# ÄÆ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c dá»¯ liá»‡uN
data_dir = "/home/nhotin/tinltn/dat301m/as/sen12floods/data/sen12floods1"

print(f"ThÆ° má»¥c dá»¯ liá»‡u: {data_dir}")

# Kiá»ƒm tra xem thÆ° má»¥c cÃ³ tá»“n táº¡i khÃ´ng
if not os.path.exists(data_dir):
    print("Lá»—i: ThÆ° má»¥c dá»¯ liá»‡u khÃ´ng tá»“n táº¡i!")
    print("Vui lÃ²ng cáº­p nháº­t Ä‘Æ°á»ng dáº«n data_dir phÃ¹ há»£p")
else:
    print("ThÆ° má»¥c dá»¯ liá»‡u tá»“n táº¡i")

print("\n" + "="*50)
print("1. KHÃM PHÃ Cáº¤U TRÃšC Dá»® LIá»†U")
print("="*50)

# Liá»‡t kÃª táº¥t cáº£ file TIFF má»™t cÃ¡ch Ä‘á»‡ quy
tif_files = glob.glob(os.path.join(data_dir, "**/*.tif"), recursive=True)
print(f"Tá»•ng sá»‘ file TIFF tÃ¬m tháº¥y: {len(tif_files):,}")

# Hiá»ƒn thá»‹ máº«u Ä‘Æ°á»ng dáº«n file Ä‘á»ƒ hiá»ƒu cáº¥u trÃºc
print("\nMáº«u Ä‘Æ°á»ng dáº«n file:")
for i, f in enumerate(tif_files[:5]):
    relative_path = f.replace(data_dir, "")
    print(f"  {i+1}. {relative_path}")

# PhÃ¢n tÃ­ch cáº¥u trÃºc thÆ° má»¥c - má»—i thÆ° má»¥c con thÆ°á»ng Ä‘áº¡i diá»‡n cho má»™t khu vá»±c/scene
folder_names = [os.path.basename(os.path.dirname(file)) for file in tif_files]
folder_counts = Counter(folder_names)

print(f"\nSá»‘ thÆ° má»¥c duy nháº¥t: {len(folder_counts)}")
print(f"ThÆ° má»¥c cÃ³ nhiá»u file nháº¥t: {folder_counts.most_common(1)[0][0]} ({folder_counts.most_common(1)[0][1]} files)")
print(f"ThÆ° má»¥c cÃ³ Ã­t file nháº¥t: {folder_counts.most_common()[-1][0]} ({folder_counts.most_common()[-1][1]} files)")

# Hiá»ƒn thá»‹ top 10 thÆ° má»¥c cÃ³ nhiá»u file nháº¥t
print(f"\nTop 10 thÆ° má»¥c cÃ³ nhiá»u file nháº¥t:")
for i, (folder, count) in enumerate(folder_counts.most_common(10)):
    print(f"  {i+1:2d}. {folder:30} - {count:3d} files")


# %% [markdown]
# ## 3. PhÃ¢n loáº¡i theo Loáº¡i Sensor
# 
# BÆ°á»›c nÃ y ráº¥t quan trá»ng Ä‘á»ƒ hiá»ƒu rÃµ vá» 2 loáº¡i sensor khÃ¡c nhau:
# 
# ### Sentinel-1 (SAR - Synthetic Aperture Radar):
# - **Radar data** - hoáº¡t Ä‘á»™ng 24/7, khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi thá»i tiáº¿t/mÃ¢y
# - **Tá»‘t cho flood detection** vÃ¬ nÆ°á»›c pháº£n xáº¡ radar signal ráº¥t Ä‘áº·c biá»‡t
# - ThÆ°á»ng cÃ³ **2 bands**: VV vÃ  VH polarization
# - **Táº§n sá»‘**: C-band (5.4 GHz)
# 
# ### Sentinel-2 (Optical Imagery):
# - **Multispectral optical** - chá»¥p áº£nh nhÆ° mÃ¡y áº£nh thÃ´ng thÆ°á»ng nhÆ°ng vá»›i nhiá»u bands
# - **13 spectral bands** - tá»« visible light Ä‘áº¿n near-infrared
# - **Äá»™ phÃ¢n giáº£i cao** - 10m, 20m, 60m tÃ¹y theo band
# - **Bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi mÃ¢y** - khÃ´ng thá»ƒ chá»¥p qua mÃ¢y dÃ y
# 
# ### Classification Logic:
# Code sáº½ phÃ¢n loáº¡i dá»±a trÃªn Ä‘Æ°á»ng dáº«n file - files cÃ³ "S1" lÃ  Sentinel-1, "S2" lÃ  Sentinel-2
# 

# %%
print("\n" + "="*50)
print("2. PHÃ‚N LOáº I THEO LOáº I SENSOR")
print("="*50)

def classify_sensor(file_path):
    """
    PhÃ¢n loáº¡i file theo loáº¡i sensor dá»±a trÃªn tÃªn file
    
    Parameters:
    -----------
    file_path : str
        ÄÆ°á»ng dáº«n Ä‘áº¿n file
        
    Returns:
    --------
    str : Loáº¡i sensor ('Sentinel-1', 'Sentinel-2', hoáº·c 'Other')
    """
    fname = os.path.basename(file_path)
    if "/S1" in file_path or fname.startswith("S1"):
        return "Sentinel-1"
    elif "/S2" in file_path or fname.startswith("S2"):
        return "Sentinel-2"
    else:
        return "Other"

# PhÃ¢n loáº¡i tá»«ng file theo sensor
sensor_types = [classify_sensor(path) for path in tif_files]

# Táº¡o DataFrame Ä‘á»ƒ dá»… phÃ¢n tÃ­ch
df_sensors = pd.DataFrame({
    "filepath": tif_files,
    "sensor": sensor_types,
    "folder": folder_names
})

# Thá»‘ng kÃª theo sensor
sensor_counts = df_sensors["sensor"].value_counts()
print("PhÃ¢n bá»‘ theo loáº¡i sensor:")
for sensor, count in sensor_counts.items():
    percentage = (count / len(tif_files)) * 100
    print(f"  {sensor:12}: {count:8,} files ({percentage:.1f}%)")

# TÃ¡ch danh sÃ¡ch theo sensor Ä‘á»ƒ phÃ¢n tÃ­ch riÃªng
sentinel1_files = df_sensors[df_sensors.sensor == "Sentinel-1"]["filepath"].tolist()
sentinel2_files = df_sensors[df_sensors.sensor == "Sentinel-2"]["filepath"].tolist()
other_files = df_sensors[df_sensors.sensor == "Other"]["filepath"].tolist()

print(f"\nChi tiáº¿t phÃ¢n loáº¡i:")
print(f"  Sentinel-1: {len(sentinel1_files):,} files")
print(f"  Sentinel-2: {len(sentinel2_files):,} files")
print(f"  Other: {len(other_files):,} files")

# Hiá»ƒn thá»‹ máº«u file names cho má»—i sensor type
if sentinel1_files:
    print(f"\nMáº«u file Sentinel-1:")
    for i, f in enumerate(sentinel1_files[:3]):
        print(f"  {os.path.basename(f)}")

if sentinel2_files:
    print(f"\nMáº«u file Sentinel-2:")
    for i, f in enumerate(sentinel2_files[:3]):
        print(f"  {os.path.basename(f)}")


# %% [markdown]
# ## 4. Táº£i vÃ  PhÃ¢n tÃ­ch Metadata
# 
# Metadata ráº¥t quan trá»ng Ä‘á»ƒ hiá»ƒu vá» labels (flood/non-flood) cá»§a tá»«ng scene:
# 
# ### Metadata Files:
# - **S1list.json**: Chá»©a metadata cho Sentinel-1 data
# - **S2list.json**: Chá»©a metadata cho Sentinel-2 data
# 
# ### Label Logic:
# - Má»—i **folder/scene** Ä‘Æ°á»£c gÃ¡n label dá»±a trÃªn metadata
# - **Label 1**: CÃ³ flooding (báº¥t ká»³ entry nÃ o trong folder cÃ³ FLOODING=True)
# - **Label 0**: KhÃ´ng cÃ³ flooding (táº¥t cáº£ entries Ä‘á»u FLOODING=False)
# 
# ### PhÃ¢n tÃ­ch Distribution:
# - Xem phÃ¢n bá»‘ giá»¯a flooded vs non-flooded scenes
# - Quan trá»ng Ä‘á»ƒ hiá»ƒu **class imbalance** trong dataset
# 

# %%
print("\n" + "="*50)
print("3. PHÃ‚N TÃCH METADATA VÃ€ LABELS")
print("="*50)

# ÄÆ°á»ng dáº«n Ä‘áº¿n file JSON chá»©a metadata
s1_json_path = os.path.join(data_dir, "S1list.json")
s2_json_path = os.path.join(data_dir, "S2list.json")

# Dictionary Ä‘á»ƒ lÆ°u flood labels cho tá»«ng folder/scene
flood_labels = {}

# Táº£i vÃ  xá»­ lÃ½ S1 metadata
if os.path.exists(s1_json_path):
    with open(s1_json_path, "r") as f:
        s1_data = json.load(f)
    print(f"ÄÃ£ táº£i S1list.json - {len(s1_data)} entries")
    
    # XÃ¢y dá»±ng mapping flood labels tá»« S1 data
    # Logic: náº¿u cÃ³ báº¥t ká»³ entry nÃ o trong folder cÃ³ FLOODING=True thÃ¬ folder Ä‘Ã³ Ä‘Æ°á»£c gÃ¡n label 1
    for folder, details in s1_data.items():
        flood_status = any(
            entry.get("FLOODING", False)
            for key, entry in details.items() 
            if isinstance(entry, dict) and "FLOODING" in entry
        )
        flood_labels[folder] = int(flood_status)
else:
    print("KhÃ´ng tÃ¬m tháº¥y S1list.json")

# Táº£i vÃ  xá»­ lÃ½ S2 metadata (tÆ°Æ¡ng tá»± S1)
if os.path.exists(s2_json_path):
    with open(s2_json_path, "r") as f:
        s2_data = json.load(f)
    print(f"ÄÃ£ táº£i S2list.json - {len(s2_data)} entries")
    
    # Cáº­p nháº­t mapping vá»›i S2 data
    for folder, details in s2_data.items():
        flood_status = any(
            entry.get("FLOODING", False)
            for key, entry in details.items() 
            if isinstance(entry, dict) and "FLOODING" in entry
        )
        flood_labels[folder] = int(flood_status)
else:
    print("KhÃ´ng tÃ¬m tháº¥y S2list.json")

print(f"\nTá»•ng sá»‘ thÆ° má»¥c cÃ³ label: {len(flood_labels)}")

# PhÃ¢n tÃ­ch distribution cá»§a labels trong toÃ n bá»™ dataset
dataset_folder_names = [os.path.basename(os.path.dirname(file)) for file in tif_files]
mapped_labels = [flood_labels.get(folder, 0) for folder in dataset_folder_names]  # Default 0 náº¿u khÃ´ng cÃ³ label
label_distribution = Counter(mapped_labels)

print("PhÃ¢n bá»‘ labels trong dataset:")
total_files = len(mapped_labels)
for label, count in label_distribution.items():
    label_name = "Flooding" if label == 1 else "Non-flooding"
    percentage = (count / total_files) * 100
    print(f"  {label_name:15}: {count:8,} files ({percentage:.1f}%)")

# Hiá»ƒn thá»‹ má»™t sá»‘ sample folders vÃ  labels
print(f"\nSample flood labels:")
sample_folders = list(flood_labels.items())[:10]
for folder, label in sample_folders:
    label_name = "Flooded" if label == 1 else "Non-flooded"
    print(f"  {folder:30} -> {label_name}")


# %% [markdown]
# ## 5. Data Cleaning - Loáº¡i bá» Invalid Files
# 
# ÄÃ¢y lÃ  bÆ°á»›c **quan trá»ng nháº¥t** Ä‘á»ƒ cáº£i thiá»‡n cháº¥t lÆ°á»£ng training:
# 
# ### Má»¥c tiÃªu Data Cleaning:
# 1. **PhÃ¡t hiá»‡n invalid files**: Files cÃ³ dá»¯ liá»‡u khÃ´ng há»£p lá»‡
# 2. **Loáº¡i bá» problematic data**: Files toÃ n NaN, toÃ n sá»‘ 0, hoáº·c corrupt
# 3. **Táº¡o clean dataset**: Chá»‰ giá»¯ láº¡i files cháº¥t lÆ°á»£ng tá»‘t
# 4. **Export clean info**: LÆ°u thÃ´ng tin Ä‘á»ƒ training script sá»­ dá»¥ng
# 
# ### CÃ¡c loáº¡i Invalid Files:
# - **All NaN**: Files chá»‰ chá»©a NaN values
# - **All zeros**: Files chá»‰ chá»©a giÃ¡ trá»‹ 0
# - **Cannot read**: Files bá»‹ corrupt hoáº·c format sai
# - **Empty files**: Files cÃ³ kÃ­ch thÆ°á»›c 0 bytes
# 
# ### Output Files:
# - `clean_dataset_info.json`: ThÃ´ng tin vá» clean dataset
# - `invalid_files_info.json`: ThÃ´ng tin vá» invalid files
# - `flood_labels_mapping.json`: Mapping tá»« folder Ä‘áº¿n flood labels
# 
# 

# %%
print("\n" + "="*50)
print("4. DATA CLEANING - LOáº I Bá» INVALID FILES")
print("="*50)

def is_valid_file(file_path):
    """
    Kiá»ƒm tra xem file cÃ³ dá»¯ liá»‡u há»£p lá»‡ hay khÃ´ng
    
    Parameters:
    -----------
    file_path : str
        ÄÆ°á»ng dáº«n Ä‘áº¿n file
        
    Returns:
    --------
    bool : True náº¿u file há»£p lá»‡, False náº¿u invalid
    """
    try:
        # Kiá»ƒm tra file tá»“n táº¡i vÃ  cÃ³ kÃ­ch thÆ°á»›c > 0
        if not os.path.exists(file_path) or os.path.getsize(file_path) == 0:
            return False
            
        with rasterio.open(file_path) as src:
            # Kiá»ƒm tra metadata cÆ¡ báº£n
            if src.width == 0 or src.height == 0 or src.count == 0:
                return False
                
            # Äá»c dá»¯ liá»‡u tá»« band Ä‘áº§u tiÃªn
            data = src.read(1).astype(float)
            
            # Kiá»ƒm tra dá»¯ liá»‡u há»£p lá»‡
            all_nan = np.all(np.isnan(data))
            all_zero = np.all(data == 0)
            
            # File invalid náº¿u toÃ n NaN hoáº·c toÃ n sá»‘ 0
            if all_nan or all_zero:
                return False
                
            return True
            
    except Exception as e:
        # Náº¿u cÃ³ lá»—i Ä‘á»c file thÃ¬ coi nhÆ° invalid
        return False

print("Äang kiá»ƒm tra tÃ­nh há»£p lá»‡ cá»§a táº¥t cáº£ files...")
print("QuÃ¡ trÃ¬nh nÃ y cÃ³ thá»ƒ máº¥t vÃ i phÃºt...")

# Táº¡o danh sÃ¡ch Ä‘á»ƒ lÆ°u káº¿t quáº£
valid_files = []
invalid_files = []
invalid_reasons = []

# Kiá»ƒm tra tá»«ng file
total_files = len(tif_files)
for i, file_path in enumerate(tif_files):
    if (i + 1) % 100 == 0:  # Progress indicator
        print(f"   ÄÃ£ kiá»ƒm tra {i+1}/{total_files} files ({(i+1)/total_files*100:.1f}%)")
    
    if is_valid_file(file_path):
        valid_files.append(file_path)
    else:
        invalid_files.append(file_path)
        # ThÃªm thÃ´ng tin chi tiáº¿t vá» lÃ½ do invalid
        try:
            with rasterio.open(file_path) as src:
                data = src.read(1).astype(float)
                if np.all(np.isnan(data)):
                    invalid_reasons.append("All NaN")
                elif np.all(data == 0):
                    invalid_reasons.append("All zeros")
                else:
                    invalid_reasons.append("Other error")
        except:
            invalid_reasons.append("Cannot read")

print(f"\nHoÃ n thÃ nh kiá»ƒm tra {total_files} files!")


# %%
# Thá»‘ng kÃª káº¿t quáº£ cleaning
print(f"\nKáº¾T QUáº¢ DATA CLEANING:")
print(f"  Valid files: {len(valid_files):,} ({len(valid_files)/total_files*100:.1f}%)")
print(f"  Invalid files: {len(invalid_files):,} ({len(invalid_files)/total_files*100:.1f}%)")

# PhÃ¢n tÃ­ch nguyÃªn nhÃ¢n invalid
if invalid_files:
    print(f"\nNGUYÃŠN NHÃ‚N INVALID:")
    reason_counts = Counter(invalid_reasons)
    for reason, count in reason_counts.items():
        print(f"  {reason}: {count:,} files")

# PhÃ¢n loáº¡i valid files theo sensor
valid_sentinel1_files = []
valid_sentinel2_files = []
valid_other_files = []

for file_path in valid_files:
    sensor = classify_sensor(file_path)
    if sensor == "Sentinel-1":
        valid_sentinel1_files.append(file_path)
    elif sensor == "Sentinel-2":
        valid_sentinel2_files.append(file_path)
    else:
        valid_other_files.append(file_path)

print(f"\nVALID FILES THEO SENSOR:")
print(f"  Sentinel-1: {len(valid_sentinel1_files):,} files")
print(f"  Sentinel-2: {len(valid_sentinel2_files):,} files")
print(f"  Other: {len(valid_other_files):,} files")

# Cáº­p nháº­t flood labels chá»‰ cho valid files
valid_folders = set()
for file_path in valid_files:
    folder_name = os.path.basename(os.path.dirname(file_path))
    valid_folders.add(folder_name)

# Táº¡o clean flood labels chá»‰ cho folders cÃ³ valid files
clean_flood_labels = {folder: label for folder, label in flood_labels.items() 
                     if folder in valid_folders}

print(f"\nFLOOD LABELS SAU CLEANING:")
clean_label_distribution = Counter(clean_flood_labels.values())
for label, count in clean_label_distribution.items():
    label_name = "Flooded" if label == 1 else "Non-flooded"
    print(f"  {label_name}: {count:,} folders")

# LÆ°u thÃ´ng tin cleaning
clean_data_file = os.path.join(data_dir, 'clean_dataset_info.json')
with open(clean_data_file, 'w') as f:
    clean_info_serializable = {
        'valid_files': valid_files,
        'valid_sentinel1_files': valid_sentinel1_files,
        'valid_sentinel2_files': valid_sentinel2_files,
        'clean_flood_labels': clean_flood_labels,
        'cleaning_stats': {
            'total_original_files': total_files,
            'valid_files_count': len(valid_files),
            'invalid_files_count': len(invalid_files),
            'cleaning_rate': len(valid_files) / total_files * 100
        }
    }
    json.dump(clean_info_serializable, f, indent=2)

print(f"\nÄÃ£ lÆ°u thÃ´ng tin clean dataset:")
print(f"{clean_data_file}")
print(f"Training script sáº½ tá»± Ä‘á»™ng load clean dataset nÃ y!")

# Táº¡o danh sÃ¡ch invalid files Ä‘á»ƒ review náº¿u cáº§n
invalid_files_info = {
    'invalid_files': invalid_files,
    'invalid_reasons': invalid_reasons
}

invalid_files_file = os.path.join(data_dir, 'invalid_files_info.json')
with open(invalid_files_file, 'w') as f:
    json.dump(invalid_files_info, f, indent=2)

# LÆ°u flood labels mapping
labels_file = os.path.join(data_dir, 'flood_labels_mapping.json')
with open(labels_file, 'w') as f:
    json.dump(clean_flood_labels, f, indent=2)

print(f"Invalid files info: {invalid_files_file}")
print(f"Flood labels mapping: {labels_file}")
print(f"All files saved to: {data_dir}")

print(f"\n CLEAN DATASET ÄÃƒ Sáº´N SÃ€NG CHO TRAINING!")
print(f" Tá»· lá»‡ data cÃ²n láº¡i: {len(valid_files)/total_files*100:.1f}%")


# %% [markdown]
# ## 6. Cáº­p nháº­t Variables cho Clean Dataset
# 
# Sau khi data cleaning, chÃºng ta cáº§n cáº­p nháº­t cÃ¡c variables Ä‘á»ƒ sá»­ dá»¥ng clean data cho cÃ¡c analysis tiáº¿p theo:
# 

# %%
# Cáº­p nháº­t cÃ¡c biáº¿n Ä‘á»ƒ sá»­ dá»¥ng clean data trong cÃ¡c cell tiáº¿p theo
print(f"\nCáº¬P NHáº¬T VARIABLES CHO CLEAN DATASET:")
print(f"  â€¢ tif_files -> valid_files ({len(valid_files):,} files)")
print(f"  â€¢ sentinel1_files -> valid_sentinel1_files ({len(valid_sentinel1_files):,} files)")
print(f"  â€¢ sentinel2_files -> valid_sentinel2_files ({len(valid_sentinel2_files):,} files)")
print(f"  â€¢ flood_labels -> clean_flood_labels ({len(clean_flood_labels)} folders)")

# Override variables vá»›i clean data
tif_files = valid_files
sentinel1_files = valid_sentinel1_files
sentinel2_files = valid_sentinel2_files
flood_labels = clean_flood_labels

# Cáº­p nháº­t DataFrame sensors vá»›i clean data
df_sensors = pd.DataFrame({
    "filepath": valid_files,
    "sensor": [classify_sensor(path) for path in valid_files],
    "folder": [os.path.basename(os.path.dirname(file)) for file in valid_files]
})

# Cáº­p nháº­t cÃ¡c thá»‘ng kÃª vá»›i clean data
folder_names = [os.path.basename(os.path.dirname(file)) for file in valid_files]
folder_counts = Counter(folder_names)

print(f"\nCLEAN DATASET ÄÃƒ Sáº´N SÃ€NG CHO ANALYSIS!")
print(f"Tá»· lá»‡ data cÃ²n láº¡i: {len(valid_files)/total_files*100:.1f}%")


# %% [markdown]
# ## 7. PhÃ¢n tÃ­ch Sentinel-2 Bands
# 
# Sentinel-2 cÃ³ 13 spectral bands vá»›i wavelengths khÃ¡c nhau, má»—i band cung cáº¥p thÃ´ng tin khÃ¡c nhau:
# 
# ### CÃ¡c Bands chÃ­nh:
# - **B02 (Blue)**: 490nm - PhÃ¡t hiá»‡n nÆ°á»›c sÃ¢u
# - **B03 (Green)**: 560nm - Vegetation peak reflection  
# - **B04 (Red)**: 665nm - Chlorophyll absorption
# - **B08 (NIR)**: 842nm - Vegetation structure
# - **B11/B12 (SWIR)**: 1610nm/2190nm - Moisture content
# 
# ### Analysis:
# Code sáº½ Ä‘áº¿m sá»‘ lÆ°á»£ng files cho má»—i band vÃ  hiá»ƒn thá»‹ wavelength information
# 

# %%
print("\n" + "="*50)
print("4. PHÃ‚N TÃCH BANDS SENTINEL-2")
print("="*50)

# TrÃ­ch xuáº¥t thÃ´ng tin band tá»« tÃªn file Sentinel-2
s2_bands = []
for file in sentinel2_files:
    fname = os.path.basename(file)
    if "_B" in fname:  # File cÃ³ format nhÆ° S2_date_B04.tif
        band = fname.split("_B")[-1].split(".")[0]  # Láº¥y pháº§n sau "_B" vÃ  trÆ°á»›c ".tif"
        s2_bands.append(band)

s2_band_counts = Counter(s2_bands)
print("PhÃ¢n bá»‘ cÃ¡c bands Sentinel-2:")
print("(Band numbers correspond to different spectral wavelengths)")

# Sáº¯p xáº¿p bands theo thá»© tá»± sá»‘
for band in sorted(s2_band_counts.keys(), key=lambda x: int(x) if x.isdigit() else 999):
    count = s2_band_counts[band]
    # ThÃªm thÃ´ng tin vá» wavelength cho tá»«ng band
    band_info = {
        '01': 'Coastal aerosol (443nm)',
        '02': 'Blue (490nm)', 
        '03': 'Green (560nm)',
        '04': 'Red (665nm)',
        '05': 'Red edge 1 (705nm)',
        '06': 'Red edge 2 (740nm)',
        '07': 'Red edge 3 (783nm)',
        '08': 'NIR (842nm)',
        '8A': 'Red edge 4 (865nm)',
        '09': 'Water vapour (945nm)',
        '11': 'SWIR 1 (1610nm)',
        '12': 'SWIR 2 (2190nm)'
    }
    description = band_info.get(band, '')
    print(f"  Band {band:3}: {count:,} files - {description}")

# Hiá»ƒn thá»‹ tá»•ng sá»‘ bands Ä‘Æ°á»£c tÃ¬m tháº¥y
print(f"\nTá»•ng sá»‘ bands khÃ¡c nhau: {len(s2_band_counts)}")
print(f"Band phá»• biáº¿n nháº¥t: B{max(s2_band_counts, key=s2_band_counts.get)} ({s2_band_counts[max(s2_band_counts, key=s2_band_counts.get)]:,} files)")


# %% [markdown]
# ## 8. Visualization Functions
# 
# Äá»‹nh nghÄ©a cÃ¡c functions Ä‘á»ƒ load vÃ  xá»­ lÃ½ images cho visualization:
# 
# ### ğŸ› ï¸ Functions bao gá»“m:
# 
# 1. **`load_s1_gray()`**: Load Sentinel-1 SAR images vÃ  convert thÃ nh grayscale 0-255
#    - Apply min-max normalization Ä‘á»ƒ tÄƒng contrast
#    - Handle exception cases
# 
# 2. **`load_s2_true_color()`**: Táº¡o true color RGB tá»« Sentinel-2 bands
#    - Tá»± Ä‘á»™ng tÃ¬m B02 (Blue), B03 (Green), B04 (Red)  
#    - Apply percentile stretch Ä‘á»ƒ cáº£i thiá»‡n contrast
#    - Return RGB array 0-255
# 
# 3. **`speckle_reduce()`**: Giáº£m speckle noise cho SAR images
#    - Sá»­ dá»¥ng median filter Ä‘á»ƒ smooth noise
#    - Quan trá»ng cho SAR data quality
# 

# %%
def load_s1_gray(fp):
    """
    Load Sentinel-1 SAR image vÃ  convert thÃ nh grayscale 0-255
    
    Parameters:
    -----------
    fp : str
        File path Ä‘áº¿n Sentinel-1 TIFF file
        
    Returns:
    --------
    numpy.ndarray : Grayscale image (H, W) vá»›i values 0-255
    """
    try:
        with rasterio.open(fp) as src:
            arr = src.read(1).astype(float)  # Äá»c band Ä‘áº§u tiÃªn
        
        # Min-max normalization Ä‘á»ƒ stretch contrast
        lo, hi = np.nanmin(arr), np.nanmax(arr)
        if hi > lo:
            arr = (arr - lo) / (hi - lo)
        else:
            arr = np.zeros_like(arr)
        return (arr * 255).astype(np.uint8)
    except Exception as e:
        print(f"Lá»—i Ä‘á»c file {fp}: {e}")
        return np.zeros((256, 256), dtype=np.uint8)

def load_s2_true_color(b04_fp):
    """
    Load Sentinel-2 true color image tá»« B04 file (tá»± Ä‘á»™ng tÃ¬m B03, B02)
    
    Parameters:
    -----------
    b04_fp : str
        File path Ä‘áº¿n B04 file cá»§a Sentinel-2
        
    Returns:
    --------
    numpy.ndarray : RGB image (H, W, 3) vá»›i values 0-255
    """
    try:
        # Tá»± Ä‘á»™ng tÃ¬m cÃ¡c bands RGB tá»« B04 filename
        base = b04_fp[:-8]  # Bá» "_B04.tif" á»Ÿ cuá»‘i
        fR = b04_fp         # B04 = Red
        fG = base + "_B03.tif"  # B03 = Green  
        fB = base + "_B02.tif"  # B02 = Blue
        
        # Kiá»ƒm tra xem táº¥t cáº£ bands cÃ³ tá»“n táº¡i khÃ´ng
        if not all(os.path.exists(f) for f in [fR, fG, fB]):
            print(f"Missing RGB bands for {b04_fp}")
            return np.zeros((256, 256, 3), dtype=np.uint8)
        
        # Äá»c 3 bands
        with rasterio.open(fR) as r, rasterio.open(fG) as g, rasterio.open(fB) as b:
            R = r.read(1).astype(float)
            G = g.read(1).astype(float)
            B = b.read(1).astype(float)
        
        # Stack thÃ nh RGB array
        out = np.stack([R, G, B], axis=-1)
        
        # Percentile stretch cho tá»«ng channel Ä‘á»ƒ tÄƒng contrast
        for i in range(3):
            ch = out[..., i]
            lo, hi = np.nanpercentile(ch, (2, 98))  # Bá» 2% extreme values
            ch = np.clip(ch, lo, hi)
            out[..., i] = (ch - lo) / (hi - lo) if hi > lo else np.zeros_like(ch)
        
        return (out * 255).astype(np.uint8)
    except Exception as e:
        print(f"Lá»—i Ä‘á»c S2 true color {b04_fp}: {e}")
        return np.zeros((256, 256, 3), dtype=np.uint8)

def speckle_reduce(img, ksize=5):
    """
    Giáº£m speckle noise cho SAR images báº±ng median filter
    
    Parameters:
    -----------
    img : numpy.ndarray
        Input SAR image
    ksize : int
        Kernel size cho median filter
        
    Returns:
    --------
    numpy.ndarray : Denoised image
    """
    return cv2.medianBlur(img.astype(np.uint8), ksize)

print("Visualization functions Ä‘Ã£ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a")


# %% [markdown]
# ## 9. Táº¡o cÃ¡c Biá»ƒu Ä‘á»“ Thá»‘ng kÃª Tá»•ng quan
# 
# Pháº§n nÃ y táº¡o comprehensive overview vá» dataset thÃ´ng qua 6 biá»ƒu Ä‘á»“ chÃ­nh:
# 
# ### CÃ¡c Biá»ƒu Ä‘á»“:
# 1. **Sensor Distribution**: PhÃ¢n bá»‘ Sentinel-1 vs Sentinel-2
# 2. **Flood Labels**: So sÃ¡nh flooded vs non-flooded
# 3. **Top Folders**: ThÆ° má»¥c cÃ³ nhiá»u files nháº¥t
# 4. **S2 Bands**: PhÃ¢n bá»‘ cÃ¡c spectral bands
# 5. **Files per Folder**: Histogram distribution 
# 6. **Label Pie Chart**: Tá»· lá»‡ pháº§n trÄƒm flooding
# 
# ### Insights cÃ³ thá»ƒ thu Ä‘Æ°á»£c:
# - **Class imbalance** trong dataset
# - **Data completeness** across folders
# - **Sensor coverage** ratio
# - **Band availability** patterns
# 

# %%
print("\n" + "="*50)
print("5. Táº O CÃC BIá»‚U Äá»’ THá»NG KÃŠ")
print("="*50)

# Cáº­p nháº­t láº¡i cÃ¡c thá»‘ng kÃª cho clean dataset
sensor_counts = df_sensors["sensor"].value_counts()
dataset_folder_names = [os.path.basename(os.path.dirname(file)) for file in tif_files]
mapped_labels = [flood_labels.get(folder, 0) for folder in dataset_folder_names]
label_distribution = Counter(mapped_labels)

# Thiáº¿t láº­p figure vá»›i multiple subplots
fig = plt.figure(figsize=(20, 15))

# Biá»ƒu Ä‘á»“ 1: PhÃ¢n bá»‘ sensor types
plt.subplot(2, 3, 1)
sensor_counts.plot(kind='bar', color=['skyblue', 'lightcoral', 'lightgreen'])
plt.title('PhÃ¢n bá»‘ Loáº¡i Sensor (Clean Dataset)', fontsize=14, fontweight='bold')
plt.ylabel('Sá»‘ lÆ°á»£ng files')
plt.xlabel('Loáº¡i Sensor')
plt.xticks(rotation=45)
# ThÃªm sá»‘ liá»‡u lÃªn cÃ¡c cá»™t
for i, v in enumerate(sensor_counts.values):
    plt.text(i, v + max(sensor_counts.values)*0.02, f'{v:,}', ha='center', fontweight='bold')

# Biá»ƒu Ä‘á»“ 2: PhÃ¢n bá»‘ flood labels
plt.subplot(2, 3, 2)
labels = ['Non-flooding', 'Flooding']
colors = ['lightblue', 'orange']
values = [label_distribution[0], label_distribution[1]]
bars = plt.bar(labels, values, color=colors)
plt.title('PhÃ¢n bá»‘ Flood Labels (Clean Dataset)', fontsize=14, fontweight='bold')
plt.ylabel('Sá»‘ lÆ°á»£ng files')
# ThÃªm pháº§n trÄƒm lÃªn cÃ¡c cá»™t
for i, v in enumerate(values):
    plt.text(i, v + max(values)*0.02, f'{v:,}\n({v/sum(values)*100:.1f}%)', 
             ha='center', fontweight='bold')

# Biá»ƒu Ä‘á»“ 3: Top 10 thÆ° má»¥c cÃ³ nhiá»u file nháº¥t
plt.subplot(2, 3, 3)
top_folders = folder_counts.most_common(10)
folders, counts = zip(*top_folders)
plt.barh(range(len(folders)), counts, color='lightgreen')
plt.title('Top 10 ThÆ° má»¥c CÃ³ Nhiá»u Files Nháº¥t', fontsize=14, fontweight='bold')
plt.xlabel('Sá»‘ lÆ°á»£ng files')
plt.yticks(range(len(folders)), folders, fontsize=8)
for i, v in enumerate(counts):
    plt.text(v + max(counts)*0.01, i, f'{v}', va='center', fontweight='bold')

# Biá»ƒu Ä‘á»“ 4: PhÃ¢n bá»‘ Sentinel-2 bands
plt.subplot(2, 3, 4)
if s2_band_counts:
    bands_sorted = sorted(s2_band_counts.keys(), key=lambda x: int(x) if x.isdigit() else 999)
    band_values = [s2_band_counts[band] for band in bands_sorted]
    bars = plt.bar(bands_sorted, band_values, color='purple', alpha=0.7)
    plt.title('PhÃ¢n bá»‘ Sentinel-2 Bands', fontsize=14, fontweight='bold')
    plt.xlabel('Band sá»‘')
    plt.ylabel('Sá»‘ lÆ°á»£ng files')
    plt.xticks(rotation=45)

# Biá»ƒu Ä‘á»“ 5: Histogram sá»‘ files per folder
plt.subplot(2, 3, 5)
files_per_folder = list(folder_counts.values())
plt.hist(files_per_folder, bins=20, color='cyan', alpha=0.7, edgecolor='black')
plt.title('PhÃ¢n bá»‘ Sá»‘ Files Má»—i ThÆ° má»¥c', fontsize=14, fontweight='bold')
plt.xlabel('Sá»‘ files per thÆ° má»¥c')
plt.ylabel('Sá»‘ lÆ°á»£ng thÆ° má»¥c')
plt.axvline(np.mean(files_per_folder), color='red', linestyle='--', 
           label=f'Trung bÃ¬nh: {np.mean(files_per_folder):.1f}')
plt.legend()

# Biá»ƒu Ä‘á»“ 6: Pie chart cho flood labels
plt.subplot(2, 3, 6)
if len(label_distribution) > 1:
    labels_pie = ['Non-flooding', 'Flooding']
    values_pie = [label_distribution[0], label_distribution[1]]
    colors_pie = ['lightblue', 'orange']
    explode = (0.05, 0.05)  # TÃ¡ch nháº¹ cÃ¡c pháº§n
    
    plt.pie(values_pie, labels=labels_pie, colors=colors_pie, explode=explode,
            autopct='%1.1f%%', startangle=90, shadow=True)
    plt.title('Tá»· lá»‡ Flooding vs Non-flooding', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

print("ÄÃ£ táº¡o táº¥t cáº£ biá»ƒu Ä‘á»“ thá»‘ng kÃª!")


# %% [markdown]
# ## 10. Visualization cÃ¡c Sample Images
# 
# Hiá»ƒn thá»‹ sample images Ä‘á»ƒ so sÃ¡nh trá»±c quan giá»¯a cÃ¡c scenario:
# 
# ### Layout Visualization:
# - **Row 1**: Sentinel-1 SAR images - Flooded areas
# - **Row 2**: Sentinel-1 SAR images - Non-flooded areas  
# - **Row 3**: Sentinel-2 True color - Flooded areas
# - **Row 4**: Sentinel-2 True color - Non-flooded areas
# 
# ### Key Observations:
# - **SAR signatures**: Flooded areas thÆ°á»ng cÃ³ **lower backscatter** (darker)
# - **Optical differences**: Flooded areas cÃ³ **water reflection** khÃ¡c biá»‡t
# - **Spatial patterns**: Flood extent vÃ  distribution patterns
# - **Sensor complementarity**: SAR penetrates clouds, Optical shows natural colors
# 
# ### Technical Notes:
# - **S1 images**: Converted to grayscale vá»›i min-max normalization
# - **S2 images**: RGB composite tá»« B02/B03/B04 vá»›i percentile stretch
# 

# %%
print("\n" + "="*50)
print("6. VISUALIZATION CÃC SAMPLES IMAGES")
print("="*50)

# Sampling Sentinel-1 images theo flood labels
s1_flooded = [fp for fp in sentinel1_files 
              if flood_labels.get(os.path.basename(os.path.dirname(fp)), 0) == 1]
s1_non = [fp for fp in sentinel1_files 
          if flood_labels.get(os.path.basename(os.path.dirname(fp)), 0) == 0]

n_samples = 5  # Sá»‘ samples hiá»ƒn thá»‹
s1_flooded_samples = random.sample(s1_flooded, min(len(s1_flooded), n_samples))
s1_non_samples = random.sample(s1_non, min(len(s1_non), n_samples))

print(f"Sentinel-1: {len(s1_flooded_samples)} flooded, {len(s1_non_samples)} non-flooded samples")

# Sampling Sentinel-2 B04 files (Ä‘á»ƒ cÃ³ thá»ƒ táº¡o true color)
s2_b04 = [fp for fp in sentinel2_files if fp.endswith("_B04.tif")]
s2_b04_flooded = [fp for fp in s2_b04 
                  if flood_labels.get(os.path.basename(os.path.dirname(fp)), 0) == 1]
s2_b04_non = [fp for fp in s2_b04 
              if flood_labels.get(os.path.basename(os.path.dirname(fp)), 0) == 0]

s2_b04_flooded_samples = random.sample(s2_b04_flooded, min(len(s2_b04_flooded), n_samples))
s2_b04_non_samples = random.sample(s2_b04_non, min(len(s2_b04_non), n_samples))

print(f"Sentinel-2: {len(s2_b04_flooded_samples)} flooded, {len(s2_b04_non_samples)} non-flooded B04 files")

# Táº¡o figure vá»›i 4 rows x 5 columns Ä‘á»ƒ hiá»ƒn thá»‹ samples
fig, axes = plt.subplots(4, 5, figsize=(20, 16))

# Row 0: Sentinel-1 flooded images
for i, fp in enumerate(s1_flooded_samples):
    img = load_s1_gray(fp)
    axes[0, i].imshow(img, cmap="gray")
    folder_name = os.path.basename(os.path.dirname(fp))
    axes[0, i].set_title(f"S1 Flooded\n{folder_name}", fontsize=10)
    axes[0, i].axis("off")

# Fill empty slots if needed
for i in range(len(s1_flooded_samples), 5):
    axes[0, i].axis("off")

# Row 1: Sentinel-1 non-flooded images  
for i, fp in enumerate(s1_non_samples):
    img = load_s1_gray(fp)
    axes[1, i].imshow(img, cmap="gray")
    folder_name = os.path.basename(os.path.dirname(fp))
    axes[1, i].set_title(f"S1 Non-Flooded\n{folder_name}", fontsize=10)
    axes[1, i].axis("off")

# Fill empty slots if needed
for i in range(len(s1_non_samples), 5):
    axes[1, i].axis("off")

# Row 2: Sentinel-2 flooded images (true color)
for i, fp in enumerate(s2_b04_flooded_samples):
    img = load_s2_true_color(fp)
    axes[2, i].imshow(img)
    folder_name = os.path.basename(os.path.dirname(fp))
    axes[2, i].set_title(f"S2 Flooded\n{folder_name}", fontsize=10)
    axes[2, i].axis("off")

# Fill empty slots if needed
for i in range(len(s2_b04_flooded_samples), 5):
    axes[2, i].axis("off")

# Row 3: Sentinel-2 non-flooded images (true color)
for i, fp in enumerate(s2_b04_non_samples):
    img = load_s2_true_color(fp)
    axes[3, i].imshow(img)
    folder_name = os.path.basename(os.path.dirname(fp))
    axes[3, i].set_title(f"S2 Non-Flooded\n{folder_name}", fontsize=10)
    axes[3, i].axis("off")

# Fill empty slots if needed
for i in range(len(s2_b04_non_samples), 5):
    axes[3, i].axis("off")

plt.suptitle('Sample Images: Sentinel-1 & Sentinel-2 (Flooded vs Non-flooded)', 
             fontsize=16, fontweight='bold', y=0.98)
plt.tight_layout()
plt.show()

print("ÄÃ£ hiá»ƒn thá»‹ sample images!")


# %% [markdown]
# ## 11. PhÃ¢n tÃ­ch KÃ­ch thÆ°á»›c vÃ  Metadata Chi tiáº¿t
# 
# PhÃ¢n tÃ­ch technical specifications cá»§a images Ä‘á»ƒ hiá»ƒu dataset á»Ÿ má»©c technical:
# 
# ### Metadata Analysis:
# - **Dimensions**: Width x Height cá»§a images
# - **Data types**: Kiá»ƒu dá»¯ liá»‡u (float32, uint16, etc.)
# - **Coordinate systems**: CRS information
# - **Band counts**: Sá»‘ bands per file
# 
# ### Outputs:
# - **Dimension histograms**: PhÃ¢n bá»‘ width/height
# - **Statistics by sensor**: So sÃ¡nh specs giá»¯a S1 vÃ  S2
# - **Common formats**: TÃ¬m format phá»• biáº¿n nháº¥t
# 
# ### Why Important:
# - **Training preparation**: Biáº¿t image sizes Ä‘á»ƒ thiáº¿t káº¿ model
# - **Memory planning**: Æ¯á»›c tÃ­nh RAM requirements  
# - **Quality checks**: PhÃ¡t hiá»‡n outliers vá» kÃ­ch thÆ°á»›c
# 

# %%
print("\n" + "="*50)
print("7. PHÃ‚N TÃCH KÃCH THÆ¯á»šC VÃ€ METADATA")
print("="*50)

# Láº¥y sample Ä‘á»ƒ phÃ¢n tÃ­ch (khÃ´ng cáº§n phÃ¢n tÃ­ch táº¥t cáº£ Ä‘á»ƒ tiáº¿t kiá»‡m thá»i gian)
sample_size = min(100, len(tif_files))
sample_files = random.sample(tif_files, sample_size)
file_info = []

print(f"Äang phÃ¢n tÃ­ch metadata tá»« {sample_size} sample files...")

for file_path in sample_files:
    try:
        with rasterio.open(file_path) as src:
            info = {
                'file': os.path.basename(file_path),
                'width': src.width,
                'height': src.height,
                'bands': src.count,
                'dtype': str(src.dtypes[0]),
                'crs': str(src.crs),
                'sensor': classify_sensor(file_path)
            }
            file_info.append(info)
    except Exception as e:
        print(f"Lá»—i Ä‘á»c metadata tá»« {file_path}: {e}")

# Táº¡o DataFrame tá»« metadata
df_info = pd.DataFrame(file_info)

if not df_info.empty:
    print("THá»NG KÃŠ KÃCH THÆ¯á»šC IMAGES:")
    print(f"  KÃ­ch thÆ°á»›c phá»• biáº¿n nháº¥t: {df_info['width'].mode().iloc[0]} x {df_info['height'].mode().iloc[0]}")
    print(f"  Sá»‘ bands trung bÃ¬nh: {df_info['bands'].mean():.1f}")
    print(f"  Data types: {df_info['dtype'].value_counts().to_dict()}")
    
    # Thá»‘ng kÃª theo sensor
    print(f"\nTHá»NG KÃŠ THEO SENSOR:")
    sensor_stats = df_info.groupby('sensor')[['width', 'height', 'bands']].agg(['mean', 'std'])
    print(sensor_stats)
    
    # Táº¡o biá»ƒu Ä‘á»“ phÃ¢n bá»‘ kÃ­ch thÆ°á»›c
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    # Histogram chiá»u rá»™ng
    df_info['width'].hist(bins=20, ax=axes[0], color='lightblue', alpha=0.7)
    axes[0].set_title('PhÃ¢n bá»‘ Chiá»u rá»™ng Images')
    axes[0].set_xlabel('Width (pixels)')
    axes[0].set_ylabel('Sá»‘ lÆ°á»£ng')
    axes[0].axvline(df_info['width'].mean(), color='red', linestyle='--', 
                   label=f'Mean: {df_info["width"].mean():.0f}')
    axes[0].legend()
    
    # Histogram chiá»u cao
    df_info['height'].hist(bins=20, ax=axes[1], color='lightcoral', alpha=0.7)
    axes[1].set_title('PhÃ¢n bá»‘ Chiá»u cao Images')
    axes[1].set_xlabel('Height (pixels)')
    axes[1].set_ylabel('Sá»‘ lÆ°á»£ng')
    axes[1].axvline(df_info['height'].mean(), color='red', linestyle='--',
                   label=f'Mean: {df_info["height"].mean():.0f}')
    axes[1].legend()
    
    plt.tight_layout()
    plt.show()
    
    print("HoÃ n thÃ nh phÃ¢n tÃ­ch metadata!")
else:
    print("KhÃ´ng thá»ƒ phÃ¢n tÃ­ch metadata tá»« sample files")


# %% [markdown]
# ## 12. Táº¡o Summary Report
# 
# Tá»•ng há»£p táº¥t cáº£ káº¿t quáº£ EDA thÃ nh bÃ¡o cÃ¡o cuá»‘i cÃ¹ng:
# 
# ### Summary Report bao gá»“m:
# 1. **Key Statistics**: Tá»•ng sá»‘ files, folders, sensors
# 2. **Class Distribution**: Tá»· lá»‡ flooding vs non-flooding
# 3. **Data Quality**: Káº¿t quáº£ sau data cleaning
# 4. **Technical Specs**: Image dimensions, formats
# 5. **Files Created**: Danh sÃ¡ch output files cho training
# 
# ### Output Files Ä‘Æ°á»£c táº¡o:
# - `clean_dataset_info.json` - Dataset Ä‘Ã£ clean cho training
# - `invalid_files_info.json` - ThÃ´ng tin vá» invalid files  
# - `flood_labels_mapping.json` - Labels mapping cho training
# 
# ### Ready for Training:
# Sau EDA nÃ y, `training` sáº½ tá»± Ä‘á»™ng detect vÃ  sá»­ dá»¥ng clean dataset!
# 

# %%
print("\n" + "="*50)
print("8. Táº O SUMMARY REPORT")
print("="*50)

# Táº¡o dictionary chá»©a táº¥t cáº£ thá»‘ng kÃª quan trá»ng
summary_stats = {
    'total_files': len(tif_files),
    'unique_folders': len(folder_counts),
    'sentinel1_files': len(sentinel1_files),
    'sentinel2_files': len(sentinel2_files),
    'other_files': len(valid_other_files),
    'flooded_samples': label_distribution[1],
    'non_flooded_samples': label_distribution[0],
    'sentinel2_bands': len(s2_band_counts),
    'max_files_per_folder': folder_counts.most_common(1)[0][1],
    'avg_files_per_folder': np.mean(list(folder_counts.values()))
}

# Táº¡o summary table
summary_data = {
    'Metric': [
        'Tá»•ng sá»‘ files TIFF (Clean)',
        'Sá»‘ thÆ° má»¥c duy nháº¥t',
        'Sentinel-1 files',
        'Sentinel-2 files',
        'Files khÃ¡c',
        'Flooded samples',
        'Non-flooded samples',
        'Sentinel-2 bands',
        'ThÆ° má»¥c cÃ³ nhiá»u files nháº¥t',
        'Files per thÆ° má»¥c (trung bÃ¬nh)'
    ],
    'Value': [
        f"{summary_stats['total_files']:,}",
        f"{summary_stats['unique_folders']:,}",
        f"{summary_stats['sentinel1_files']:,}",
        f"{summary_stats['sentinel2_files']:,}",
        f"{summary_stats['other_files']:,}",
        f"{summary_stats['flooded_samples']:,}",
        f"{summary_stats['non_flooded_samples']:,}",
        f"{summary_stats['sentinel2_bands']}",
        f"{summary_stats['max_files_per_folder']}",
        f"{summary_stats['avg_files_per_folder']:.1f}"
    ]
}

summary_df = pd.DataFrame(summary_data)

print("SUMMARY REPORT:")
print(summary_df.to_string(index=False))

# Táº¡o additional insights
print(f"\nADDITIONAL INSIGHTS:")
print(f"  â€¢ Dataset cÃ³ {summary_stats['flooded_samples']/(summary_stats['flooded_samples']+summary_stats['non_flooded_samples'])*100:.1f}% flooding samples")
print(f"  â€¢ Sentinel-2 chiáº¿m {summary_stats['sentinel2_files']/summary_stats['total_files']*100:.1f}% tá»•ng sá»‘ files")
print(f"  â€¢ Trung bÃ¬nh má»—i folder cÃ³ {summary_stats['avg_files_per_folder']:.1f} files")
print(f"  â€¢ CÃ³ {summary_stats['sentinel2_bands']} bands khÃ¡c nhau trong Sentinel-2")
print(f"  â€¢ Class imbalance ratio: {summary_stats['non_flooded_samples']/summary_stats['flooded_samples']:.1f}:1 (non-flood:flood)")

# Export cÃ¡c variables quan trá»ng cho sá»­ dá»¥ng sau nÃ y
print(f"\nCÃC VARIABLES ÄÃƒ Táº O:")
print(f"  â€¢ tif_files: List of all clean TIFF file paths")
print(f"  â€¢ flood_labels: Dictionary mapping folder -> flood label")
print(f"  â€¢ sentinel1_files, sentinel2_files: Lists phÃ¢n theo sensor")
print(f"  â€¢ df_sensors: DataFrame chá»©a thÃ´ng tin táº¥t cáº£ files")
print(f"  â€¢ summary_stats: Dictionary chá»©a táº¥t cáº£ thá»‘ng kÃª")

print(f"\nFILES ÄÃƒ Táº O:")
print(f"  â€¢ clean_dataset_info.json - Clean dataset cho training")
print(f"  â€¢ invalid_files_info.json - Danh sÃ¡ch invalid files")
print(f"  â€¢ flood_labels_mapping.json - Labels mapping cho training")
print(f"Táº¥t cáº£ files Ä‘Ã£ lÆ°u táº¡i: {data_dir}")

print("\n" + "="*60)
print("HOÃ€N THÃ€NH EXPLORATORY DATA ANALYSIS!")
print("="*60)
print("Dataset Ä‘Ã£ Ä‘Æ°á»£c cleaned vÃ  prepared")
print("Metadata vÃ  labels Ä‘Ã£ Ä‘Æ°á»£c analyzed")  
print("Visualization Ä‘Ã£ Ä‘Æ°á»£c táº¡o")
print("Summary statistics Ä‘Ã£ Ä‘Æ°á»£c calculated")
print("Sáº´N SÃ€NG CHO TRAINING vá»›i tensorflow_training.py!")
print("="*60)



